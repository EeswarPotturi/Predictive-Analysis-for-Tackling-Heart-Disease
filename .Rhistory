str(df)
# Select relevant features (adjust based on actual column names)
# Use only the columns that exist in the dataframe
selected_columns <- c("sex", "cp", "sample_origin", "patient_cohort", "stage", "target")
selected_columns <- selected_columns[selected_columns %in% colnames(df)]  # Filter to existing columns
# Create a new dataframe with selected columns
df <- df[, selected_columns]
# Normalize the dataset (scale numeric features)
df[, -which(names(df) == "target")] <- scale(df[, -which(names(df) == "target")])  # Exclude target for scaling
# Set seed for reproducibility
set.seed(1234)
# Split data into training and testing sets
pd <- sample(2, nrow(df), replace = TRUE, prob = c(0.75, 0.25))
tr <- df[pd == 1, ]
ts <- df[pd == 2, ]
# Ensure lengths match before training KNN
if (nrow(tr) > 0 && nrow(ts) > 0) {
# Train the KNN classifier
k_value <- 5  # You can tune this value for better performance
classifier_knn <- knn(train = tr[, -which(names(tr) == "target")],   # Exclude the target variable
test = ts[, -which(names(ts) == "target")],     # Exclude the target variable
cl = tr$target,                                  # Class labels
k = k_value)
# Confusion Matrix
cm <- table(ts$target, classifier_knn)
print(cm)
# Calculate accuracy
accuracy <- sum(diag(cm)) / sum(cm)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
# Plot confusion matrix heatmap
# Plot confusion matrix heatmap
cm_melted <- melt(cm)
colnames(cm_melted) <- c("True", "Predicted", "Count")  # Rename columns for clarity
ggplot(data = cm_melted, aes(x = Predicted, y = True, fill = Count)) +
geom_tile(color = "white") +
scale_fill_gradient(low = "lightblue", high = "darkblue") +
labs(x = "Predicted Label", y = "True Label", fill = "Count") +
ggtitle(paste("Confusion Matrix (Accuracy:", round(accuracy * 100, 2), "%)")) +
theme_minimal()
} else {
print("Training or test dataset is empty!")
}
#SVM
# Load necessary libraries
library(caTools)
library(e1071)
library(ggplot2)  # For the pie chart
# Load the heart disease dataset
df <- read.csv("heart-disease.csv")
# Check the structure of the dataset to confirm column names and data types
str(df)
# Convert categorical columns to factors or integers as needed
df$sex <- as.integer(as.factor(df$sex))
df$cp <- as.integer(as.factor(df$cp))         # Example of a categorical column
df$target <- as.factor(df$target)             # Target variable
# Drop any unnecessary columns (e.g., ID column if present)
df <- df[, -c(1)]  # Adjust as needed based on your dataset
# Set seed for reproducibility
set.seed(777)
# Split data into training and testing sets
split <- sample.split(df$target, SplitRatio = 0.8)
train_data <- subset(df, split == TRUE)
test_data <- subset(df, split == FALSE)
# Check structures to ensure they match
str(train_data)
str(test_data)
# Train the SVM classifier with a linear kernel
classifier <- svm(formula = target ~ .,
data = train_data,
type = 'C-classification',
kernel = 'linear')
# Predict on test set
predictions <- predict(classifier, newdata = test_data)
# Generate a confusion matrix
cm <- table(predictions, test_data$target)
print(cm)
# Calculate accuracy
accuracy <- sum(diag(cm)) / sum(cm)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
# Pie chart to visualize the distribution of predictions and actual targets
cm_df <- as.data.frame(cm)
colnames(cm_df) <- c("Predicted", "Actual", "Count")
# Plot the pie chart for the confusion matrix
ggplot(cm_df, aes(x = "", y = Count, fill = interaction(Predicted, Actual))) +
geom_bar(stat = "identity", width = 1) +
coord_polar(theta = "y") +
theme_void() +
labs(title = "Pie Chart of Confusion Matrix (Predicted vs Actual)")
# Load necessary libraries
library(rpart)
library(rpart.plot)
# Load the dataset
df <- read.csv("heart-disease.csv")
# Display structure to confirm column names and types
str(df)
# Update with the actual target column name (e.g., 'target' if it's different from 'diagnosis')
target_column <- "target"  # Replace with actual name based on str(df)
# Convert relevant columns to factors if they are categorical (based on actual structure)
df$sex <- as.factor(df$sex)
# Split the data into training and testing sets (80% train, 20% test)
set.seed(1234)
pd <- sample(2, nrow(df), replace = TRUE, prob = c(0.8, 0.2))
tr <- df[pd == 1, ]
ts <- df[pd == 2, ]
# Define formula dynamically based on target column
formula <- as.formula(paste(target_column, "~ ."))
# Train the decision tree model on training data
dstree1 <- rpart(formula, data = tr, method = "class")
# Plot the decision tree
rpart.plot(dstree1)
# Predict on test data
predictions <- predict(dstree1, ts, type = "class")
# Confusion matrix and accuracy calculation
tab <- table(predictions, ts[[target_column]])
accuracy <- sum(diag(tab)) / sum(tab)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
# Install required library (if not already installed)
# install.packages("randomForest")
library(randomForest)
# Load the dataset
heart_data <- read.csv("heart-disease.csv")
# Check the structure and summary of the dataset
str(heart_data)
summary(heart_data)
# Convert categorical columns to factors
heart_data$sex <- as.factor(heart_data$sex)
heart_data$cp <- as.factor(heart_data$cp)  # Example: Chest pain type
heart_data$target <- as.factor(heart_data$target)  # Target variable (e.g., presence of heart disease)
# Set a seed for reproducibility
set.seed(1234)
# Fit the random forest model
# Replace 'target' with the actual name of the target column in your dataset
random_forest_model <- randomForest(target ~ age + sex + cp + trestbps + chol + thalach,
data = heart_data,
ntree = 100,  # Number of trees
mtry = 3,    # Number of variables randomly sampled at each split
importance = TRUE)
# Print model summary
print(random_forest_model)
# Plot variable importance
varImpPlot(random_forest_model, main = "Variable Importance in Random Forest for Heart Disease Prediction")
#knn
# Load necessary libraries
library(caTools)   # For splitting the data
library(class)     # For KNN
library(ggplot2)   # For plotting
library(reshape2)  # For reshaping data for plotting
# Load the dataset
df <- read.csv("heart-disease.csv")  # Ensure this path is correct
# Check the structure of the dataset
str(df)
# Print column names to verify what is available
print(colnames(df))
# Check for missing columns before conversion
if ("sex" %in% colnames(df)) {
df$sex <- as.numeric(as.factor(df$sex))
}
if ("cp" %in% colnames(df)) {
df$cp <- as.numeric(as.factor(df$cp))  # Chest pain type
}
if ("target" %in% colnames(df)) {
df$target <- as.factor(df$target)        # Target variable (heart disease presence)
}
if ("sample_origin" %in% colnames(df)) {
df$sample_origin <- as.numeric(as.factor(df$sample_origin))
}
if ("patient_cohort" %in% colnames(df)) {
df$patient_cohort <- as.numeric(as.factor(df$patient_cohort))
}
if ("stage" %in% colnames(df)) {
df$stage <- as.numeric(as.factor(df$stage))
}
# Check structure again after preprocessing
str(df)
# Select relevant features (adjust based on actual column names)
# Use only the columns that exist in the dataframe
selected_columns <- c("sex", "cp", "sample_origin", "patient_cohort", "stage", "target")
selected_columns <- selected_columns[selected_columns %in% colnames(df)]  # Filter to existing columns
# Create a new dataframe with selected columns
df <- df[, selected_columns]
# Normalize the dataset (scale numeric features)
df[, -which(names(df) == "target")] <- scale(df[, -which(names(df) == "target")])  # Exclude target for scaling
# Set seed for reproducibility
set.seed(1234)
# Split data into training and testing sets
pd <- sample(2, nrow(df), replace = TRUE, prob = c(0.75, 0.25))
tr <- df[pd == 1, ]
ts <- df[pd == 2, ]
# Ensure lengths match before training KNN
if (nrow(tr) > 0 && nrow(ts) > 0) {
# Train the KNN classifier
k_value <- 5  # You can tune this value for better performance
classifier_knn <- knn(train = tr[, -which(names(tr) == "target")],   # Exclude the target variable
test = ts[, -which(names(ts) == "target")],     # Exclude the target variable
cl = tr$target,                                  # Class labels
k = k_value)
# Confusion Matrix
cm <- table(ts$target, classifier_knn)
print(cm)
# Calculate accuracy
accuracy <- sum(diag(cm)) / sum(cm)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
# Plot confusion matrix heatmap
# Plot confusion matrix heatmap
cm_melted <- melt(cm)
colnames(cm_melted) <- c("True", "Predicted", "Count")  # Rename columns for clarity
ggplot(data = cm_melted, aes(x = Predicted, y = True, fill = Count)) +
geom_tile(color = "white") +
scale_fill_gradient(low = "lightblue", high = "darkblue") +
labs(x = "Predicted Label", y = "True Label", fill = "Count") +
ggtitle(paste("Confusion Matrix (Accuracy:", round(accuracy * 100, 2), "%)")) +
theme_minimal()
} else {
print("Training or test dataset is empty!")
}
library(dplyr)
library(ggplot2)
library(corrplot)
# Load the dataset
heart_data <- read.csv("heart-disease.csv")
# Check structure of the dataset
str(heart_data)
summary(heart_data)
# Convert categorical columns to factors
heart_data$sex <- as.factor(heart_data$sex)
heart_data$cp <- as.factor(heart_data$cp)  # Example: Chest pain type
# Select a continuous variable for regression (e.g., 'chol' for cholesterol or 'thalach' for max heart rate)
# Replace 'chol' with your chosen target column if different
target_column <- "chol"  # Replace with the name of the continuous target column
# Define the formula for linear regression
linear_formula <- as.formula(paste(target_column, "~ age + sex + cp + trestbps"))
# Train the linear regression model
linear_model <- lm(linear_formula, data = heart_data)
# Display summary of the linear model
summary(linear_model)
# Visualize correlations between features
corr_matrix <- cor(heart_data %>% select_if(is.numeric))
corrplot(corr_matrix, method = "circle", type = "upper", tl.col = "black", tl.srt = 45)
# Plot the relationship between target variable and one predictor, e.g., 'age' vs 'chol'
ggplot(heart_data, aes(x = age, y = chol)) +
geom_point(aes(color = cp), alpha = 0.5) +
geom_smooth(method = "lm", se = FALSE, color = "blue") +
labs(title = "Linear Regression: Cholesterol vs Age",
x = "Age",
y = "Cholesterol") +
theme_minimal()
#knn
# Load necessary libraries
library(caTools)   # For splitting the data
library(class)     # For KNN
library(ggplot2)   # For plotting
library(reshape2)  # For reshaping data for plotting
# Load the dataset
df <- read.csv("heart-disease.csv")  # Ensure this path is correct
# Check the structure of the dataset
str(df)
# Print column names to verify what is available
print(colnames(df))
# Check for missing columns before conversion
if ("sex" %in% colnames(df)) {
df$sex <- as.numeric(as.factor(df$sex))
}
if ("cp" %in% colnames(df)) {
df$cp <- as.numeric(as.factor(df$cp))  # Chest pain type
}
if ("target" %in% colnames(df)) {
df$target <- as.factor(df$target)        # Target variable (heart disease presence)
}
if ("sample_origin" %in% colnames(df)) {
df$sample_origin <- as.numeric(as.factor(df$sample_origin))
}
if ("patient_cohort" %in% colnames(df)) {
df$patient_cohort <- as.numeric(as.factor(df$patient_cohort))
}
if ("stage" %in% colnames(df)) {
df$stage <- as.numeric(as.factor(df$stage))
}
# Check structure again after preprocessing
str(df)
# Select relevant features (adjust based on actual column names)
# Use only the columns that exist in the dataframe
selected_columns <- c("sex", "cp", "sample_origin", "patient_cohort", "stage", "target")
selected_columns <- selected_columns[selected_columns %in% colnames(df)]  # Filter to existing columns
# Create a new dataframe with selected columns
df <- df[, selected_columns]
# Normalize the dataset (scale numeric features)
df[, -which(names(df) == "target")] <- scale(df[, -which(names(df) == "target")])  # Exclude target for scaling
# Set seed for reproducibility
set.seed(1234)
# Split data into training and testing sets
pd <- sample(2, nrow(df), replace = TRUE, prob = c(0.75, 0.25))
tr <- df[pd == 1, ]
ts <- df[pd == 2, ]
# Ensure lengths match before training KNN
if (nrow(tr) > 0 && nrow(ts) > 0) {
# Train the KNN classifier
k_value <- 5  # You can tune this value for better performance
classifier_knn <- knn(train = tr[, -which(names(tr) == "target")],   # Exclude the target variable
test = ts[, -which(names(ts) == "target")],     # Exclude the target variable
cl = tr$target,                                  # Class labels
k = k_value)
# Confusion Matrix
cm <- table(ts$target, classifier_knn)
print(cm)
# Calculate accuracy
accuracy <- sum(diag(cm)) / sum(cm)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
# Plot confusion matrix heatmap
# Plot confusion matrix heatmap
cm_melted <- melt(cm)
colnames(cm_melted) <- c("True", "Predicted", "Count")  # Rename columns for clarity
ggplot(data = cm_melted, aes(x = Predicted, y = True, fill = Count)) +
geom_tile(color = "white") +
scale_fill_gradient(low = "lightblue", high = "darkblue") +
labs(x = "Predicted Label", y = "True Label", fill = "Count") +
ggtitle(paste("Confusion Matrix (Accuracy:", round(accuracy * 100, 2), "%)")) +
theme_minimal()
} else {
print("Training or test dataset is empty!")
}
#SVM
# Load necessary libraries
library(caTools)
library(e1071)
library(ggplot2)  # For the pie chart
# Load the heart disease dataset
df <- read.csv("heart-disease.csv")
# Check the structure of the dataset to confirm column names and data types
str(df)
# Convert categorical columns to factors or integers as needed
df$sex <- as.integer(as.factor(df$sex))
df$cp <- as.integer(as.factor(df$cp))         # Example of a categorical column
df$target <- as.factor(df$target)             # Target variable
# Drop any unnecessary columns (e.g., ID column if present)
df <- df[, -c(1)]  # Adjust as needed based on your dataset
# Set seed for reproducibility
set.seed(777)
# Split data into training and testing sets
split <- sample.split(df$target, SplitRatio = 0.8)
train_data <- subset(df, split == TRUE)
test_data <- subset(df, split == FALSE)
# Check structures to ensure they match
str(train_data)
str(test_data)
# Train the SVM classifier with a linear kernel
classifier <- svm(formula = target ~ .,
data = train_data,
type = 'C-classification',
kernel = 'linear')
# Predict on test set
predictions <- predict(classifier, newdata = test_data)
# Generate a confusion matrix
cm <- table(predictions, test_data$target)
print(cm)
# Calculate accuracy
accuracy <- sum(diag(cm)) / sum(cm)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
# Pie chart to visualize the distribution of predictions and actual targets
cm_df <- as.data.frame(cm)
colnames(cm_df) <- c("Predicted", "Actual", "Count")
# Plot the pie chart for the confusion matrix
ggplot(cm_df, aes(x = "", y = Count, fill = interaction(Predicted, Actual))) +
geom_bar(stat = "identity", width = 1) +
coord_polar(theta = "y") +
theme_void() +
labs(title = "Pie Chart of Confusion Matrix (Predicted vs Actual)")
# Load necessary libraries
library(rpart)
library(rpart.plot)
# Load the dataset
df <- read.csv("heart-disease.csv")
# Display structure to confirm column names and types
str(df)
# Update with the actual target column name (e.g., 'target' if it's different from 'diagnosis')
target_column <- "target"  # Replace with actual name based on str(df)
# Convert relevant columns to factors if they are categorical (based on actual structure)
df$sex <- as.factor(df$sex)
# Split the data into training and testing sets (80% train, 20% test)
set.seed(1234)
pd <- sample(2, nrow(df), replace = TRUE, prob = c(0.8, 0.2))
tr <- df[pd == 1, ]
ts <- df[pd == 2, ]
# Define formula dynamically based on target column
formula <- as.formula(paste(target_column, "~ ."))
# Train the decision tree model on training data
dstree1 <- rpart(formula, data = tr, method = "class")
# Plot the decision tree
rpart.plot(dstree1)
# Predict on test data
predictions <- predict(dstree1, ts, type = "class")
# Confusion matrix and accuracy calculation
tab <- table(predictions, ts[[target_column]])
accuracy <- sum(diag(tab)) / sum(tab)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
#SVM
# Load necessary libraries
library(caTools)
library(e1071)
library(ggplot2)  # For the pie chart
# Load the heart disease dataset
df <- read.csv("heart-disease.csv")
# Check the structure of the dataset to confirm column names and data types
str(df)
# Convert categorical columns to factors or integers as needed
df$sex <- as.integer(as.factor(df$sex))
df$cp <- as.integer(as.factor(df$cp))         # Example of a categorical column
df$target <- as.factor(df$target)             # Target variable
# Drop any unnecessary columns (e.g., ID column if present)
df <- df[, -c(1)]  # Adjust as needed based on your dataset
# Set seed for reproducibility
set.seed(777)
# Split data into training and testing sets
split <- sample.split(df$target, SplitRatio = 0.8)
train_data <- subset(df, split == TRUE)
test_data <- subset(df, split == FALSE)
# Check structures to ensure they match
str(train_data)
str(test_data)
# Train the SVM classifier with a linear kernel
classifier <- svm(formula = target ~ .,
data = train_data,
type = 'C-classification',
kernel = 'linear')
# Predict on test set
predictions <- predict(classifier, newdata = test_data)
# Generate a confusion matrix
cm <- table(predictions, test_data$target)
print(cm)
# Calculate accuracy
accuracy <- sum(diag(cm)) / sum(cm)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
# Pie chart to visualize the distribution of predictions and actual targets
cm_df <- as.data.frame(cm)
colnames(cm_df) <- c("Predicted", "Actual", "Count")
# Plot the pie chart for the confusion matrix
ggplot(cm_df, aes(x = "", y = Count, fill = interaction(Predicted, Actual))) +
geom_bar(stat = "identity", width = 1) +
coord_polar(theta = "y") +
theme_void() +
labs(title = "Pie Chart of Confusion Matrix (Predicted vs Actual)")
library(dplyr)
library(ggplot2)
library(corrplot)
# Load the dataset
heart_data <- read.csv("heart-disease.csv")
# Check structure of the dataset
str(heart_data)
summary(heart_data)
# Convert categorical columns to factors
heart_data$sex <- as.factor(heart_data$sex)
heart_data$cp <- as.factor(heart_data$cp)  # Example: Chest pain type
# Select a continuous variable for regression (e.g., 'chol' for cholesterol or 'thalach' for max heart rate)
# Replace 'chol' with your chosen target column if different
target_column <- "chol"  # Replace with the name of the continuous target column
# Define the formula for linear regression
linear_formula <- as.formula(paste(target_column, "~ age + sex + cp + trestbps"))
# Train the linear regression model
linear_model <- lm(linear_formula, data = heart_data)
# Display summary of the linear model
summary(linear_model)
# Visualize correlations between features
corr_matrix <- cor(heart_data %>% select_if(is.numeric))
corrplot(corr_matrix, method = "circle", type = "upper", tl.col = "black", tl.srt = 45)
# Plot the relationship between target variable and one predictor, e.g., 'age' vs 'chol'
ggplot(heart_data, aes(x = age, y = chol)) +
geom_point(aes(color = cp), alpha = 0.5) +
geom_smooth(method = "lm", se = FALSE, color = "blue") +
labs(title = "Linear Regression: Cholesterol vs Age",
x = "Age",
y = "Cholesterol") +
theme_minimal()
# Visualize correlations between features
corr_matrix <- cor(heart_data %>% select_if(is.numeric))
corrplot(corr_matrix, method = "circle", type = "upper", tl.col = "black", tl.srt = 45)
# Visualize correlations between features
corr_matrix <- cor(heart_data %>% select_if(is.numeric))
corrplot(corr_matrix, method = "circle", type = "upper", tl.col = "black", tl.srt = 45)
# Plot the relationship between target variable and one predictor, e.g., 'age' vs 'chol'
ggplot(heart_data, aes(x = age, y = chol)) +
geom_point(aes(color = cp), alpha = 0.5) +
geom_smooth(method = "lm", se = FALSE, color = "blue") +
labs(title = "Linear Regression: Cholesterol vs Age",
x = "Age",
y = "Cholesterol") +
theme_minimal()
# Display summary of the linear model
summary(linear_model)
# Visualize correlations between features
corr_matrix <- cor(heart_data %>% select_if(is.numeric))
corrplot(corr_matrix, method = "circle", type = "upper", tl.col = "black", tl.srt = 45)
# Train the linear regression model
linear_model <- lm(linear_formula, data = heart_data)
# Display summary of the linear model
summary(linear_model)
# Visualize correlations between features
corr_matrix <- cor(heart_data %>% select_if(is.numeric))
corrplot(corr_matrix, method = "circle", type = "upper", tl.col = "black", tl.srt = 45)
# Plot the relationship between target variable and one predictor, e.g., 'age' vs 'chol'
ggplot(heart_data, aes(x = age, y = chol)) +
geom_point(aes(color = cp), alpha = 0.5) +
geom_smooth(method = "lm", se = FALSE, color = "blue") +
labs(title = "Linear Regression: Cholesterol vs Age",
x = "Age",
y = "Cholesterol") +
theme_minimal()
# Train the linear regression model
linear_model <- lm(linear_formula, data = heart_data)
# Display summary of the linear model
summary(linear_model)
# Visualize correlations between features
corr_matrix <- cor(heart_data %>% select_if(is.numeric))
corrplot(corr_matrix, method = "circle", type = "upper", tl.col = "black", tl.srt = 45)
